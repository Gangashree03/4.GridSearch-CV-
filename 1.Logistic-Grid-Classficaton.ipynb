{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"Social_Network_Ads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.get_dummies(dataset,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    257\n",
       "1    143\n",
       "Name: Purchased, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Purchased\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User ID', 'Age', 'EstimatedSalary', 'Purchased', 'Gender_Male'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep=dataset[[ 'Age', 'EstimatedSalary','Gender_Male' ]]\n",
    "dep=dataset[ 'Purchased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(indep, dep, test_size = 1/3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc= StandardScaler()\n",
    "x_train=sc.fit_transform(X_train)\n",
    "x_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid=({'penalty': ['l2'],\n",
       "                          'solver': ['newton-cg', 'lbfgs', 'liblinear',\n",
       "                                     'saga']},),\n",
       "             scoring='f1_weighted', verbose=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "              'penalty':[ 'l2']},\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, refit = True, verbose = 3,n_jobs=-1,scoring='f1_weighted')\n",
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELCOT\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:435: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    }
   ],
   "source": [
    "re=grid.cv_results_\n",
    "grid_predictions = grid.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, grid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELCOT\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ELCOT\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ELCOT\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "clf_report = classification_report(y_test, grid_predictions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 macro value for parameter {'penalty': 'l2', 'solver': 'liblinear'}: 0.1958241578990294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_macro=f1_score(y_test, grid_predictions,average='weighted')\n",
    "print(\"The f1 macro value for parameter {}:\".format(grid.best_params_),f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 85]\n",
      " [ 0 49]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        85\n",
      "           1       0.37      1.00      0.54        49\n",
      "\n",
      "    accuracy                           0.37       134\n",
      "   macro avg       0.18      0.50      0.27       134\n",
      "weighted avg       0.13      0.37      0.20       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024008</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'penalty': 'l2', 'solver': 'newton-cg'}</td>\n",
       "      <td>0.799620</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.68205</td>\n",
       "      <td>0.92351</td>\n",
       "      <td>0.922185</td>\n",
       "      <td>0.827737</td>\n",
       "      <td>0.089864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015483</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>0.006904</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'penalty': 'l2', 'solver': 'lbfgs'}</td>\n",
       "      <td>0.799620</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.68205</td>\n",
       "      <td>0.92351</td>\n",
       "      <td>0.922185</td>\n",
       "      <td>0.827737</td>\n",
       "      <td>0.089864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'penalty': 'l2', 'solver': 'liblinear'}</td>\n",
       "      <td>0.804764</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.68205</td>\n",
       "      <td>0.92351</td>\n",
       "      <td>0.922185</td>\n",
       "      <td>0.828766</td>\n",
       "      <td>0.089565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.004864</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>{'penalty': 'l2', 'solver': 'saga'}</td>\n",
       "      <td>0.799620</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.68205</td>\n",
       "      <td>0.92351</td>\n",
       "      <td>0.922185</td>\n",
       "      <td>0.827737</td>\n",
       "      <td>0.089864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_penalty  \\\n",
       "0       0.024008      0.005242         0.003066        0.004749            l2   \n",
       "1       0.015483      0.004370         0.006904        0.003694            l2   \n",
       "2       0.003722      0.002612         0.007212        0.001882            l2   \n",
       "3       0.003897      0.003318         0.004864        0.004750            l2   \n",
       "\n",
       "  param_solver                                    params  split0_test_score  \\\n",
       "0    newton-cg  {'penalty': 'l2', 'solver': 'newton-cg'}           0.799620   \n",
       "1        lbfgs      {'penalty': 'l2', 'solver': 'lbfgs'}           0.799620   \n",
       "2    liblinear  {'penalty': 'l2', 'solver': 'liblinear'}           0.804764   \n",
       "3         saga       {'penalty': 'l2', 'solver': 'saga'}           0.799620   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.811321            0.68205            0.92351           0.922185   \n",
       "1           0.811321            0.68205            0.92351           0.922185   \n",
       "2           0.811321            0.68205            0.92351           0.922185   \n",
       "3           0.811321            0.68205            0.92351           0.922185   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.827737        0.089864                2  \n",
       "1         0.827737        0.089864                2  \n",
       "2         0.828766        0.089565                1  \n",
       "3         0.827737        0.089864                2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.DataFrame.from_dict(re)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
